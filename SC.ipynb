{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SC.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/estarriol1999/AI-cup/blob/master/SC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE_IJtHnBrkp",
        "colab_type": "code",
        "outputId": "ee75bd0b-3d90-4368-e775-62a430ad857d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/SC')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "MIR-ST500  ST_1.pt  ST_3.pt  ST_5.pt  ST_7.pt  train_11.pkl\n",
            "ST_0.pt    ST_2.pt  ST_4.pt  ST_6.pt  ST_8.pt  train_all.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C4HunEKCWbw",
        "colab_type": "code",
        "outputId": "5119ca17-7a1c-4512-c844-ebd4603c4141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "class Myrnn(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size= 100):\n",
        "        super(Myrnn, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Linear1 = nn.Linear(input_dim, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers= 5, bidirectional= True)\n",
        "        self.Linear2 = nn.Linear(hidden_size* 2, 2)\n",
        "        self.Linear3 = nn.Linear(hidden_size* 2, 1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        out = F.relu(self.Linear1(input_data))\n",
        "        out, hidden = self.rnn(out)\n",
        "        #out1 is for onset & offset\n",
        "        out1 = torch.sigmoid(self.Linear2(out))\n",
        "        #out2 is for pitch\n",
        "        out2 = self.Linear3(out)\n",
        "        return out1, out2\n",
        "\n",
        "class MyData(Data.Dataset):\n",
        "    def __init__(self, data_seq, label):\n",
        "        self.data_seq = data_seq\n",
        "        self.label= label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_seq)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'data': self.data_seq[idx],\n",
        "            'label': self.label[idx]\n",
        "        }\n",
        "\n",
        "def collate_fn(samples):\n",
        "    batch = {}\n",
        "    #print (samples[0]['data'].shape)\n",
        "    temp= [torch.from_numpy(np.array(sample['data'], dtype= np.float32)) for sample in samples]\n",
        "    padded_data = rnn_utils.pad_sequence(temp, batch_first=True, padding_value= 0)\n",
        "    batch['data']= padded_data\n",
        "    batch['label']= [np.array(sample['label'], dtype= np.float32) for sample in samples]\n",
        "\n",
        "    return batch\n",
        "\n",
        "def post_processing(output1, pitch):\n",
        "    pitch= pitch.squeeze(1).squeeze(1).cpu().detach().numpy()\n",
        "    print (pitch.shape)\n",
        "    print (torch.mean(output1))\n",
        "    threshold= 0.1\n",
        "    notes= []\n",
        "    this_onset= None\n",
        "    this_offset= None\n",
        "    this_pitch= None\n",
        "\n",
        "    for i in range(len(output1)):\n",
        "        if output1[i][0][0] > threshold and this_onset == None:\n",
        "            this_onset= i\n",
        "        elif output1[i][0][1] > threshold and this_onset != None and this_onset+ 1 < i and this_offset == None:\n",
        "            this_offset= i\n",
        "            this_pitch= int(round(np.mean(pitch[this_onset:this_offset+ 1])))\n",
        "            notes.append([this_onset* 0.032+ 0.016, this_offset* 0.032+ 0.016, this_pitch])\n",
        "            this_onset= None\n",
        "            this_offset= None\n",
        "            this_pitch= None\n",
        "\n",
        "    print (np.array(notes))\n",
        "    return notes\n",
        "\n",
        "def testing(net, sample, device):\n",
        "    net.eval()\n",
        "    data = sample['data']\n",
        "    data= torch.Tensor(data)\n",
        "\n",
        "    target= sample['label']\n",
        "    target= torch.Tensor(target)\n",
        "\n",
        "    data= data.unsqueeze(1)\n",
        "    target= target.unsqueeze(1)\n",
        "\n",
        "    print (data.shape)\n",
        "    print (target.shape)\n",
        "\n",
        "    data_length= list(data.shape)[0]\n",
        "\n",
        "    data = data.to(device, dtype=torch.float)\n",
        "    target = target.to(device, dtype=torch.float)\n",
        "\n",
        "    output1, output2 = net(data)\n",
        "    print (output1.shape)\n",
        "    print (output2.shape)\n",
        "    answer = post_processing(output1, output2)\n",
        "    return answer\n",
        "\n",
        "def do_training(net, loader, optimizer, device):\n",
        "\n",
        "    num_epoch = 50\n",
        "    criterion_onset= nn.BCELoss()\n",
        "    criterion_pitch= nn.L1Loss()\n",
        "    train_loss= 0.0\n",
        "    total_length= 0\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        net.train()\n",
        "        total_length= 0.0\n",
        "        print (\"epoch %d start time: %f\" %(epoch, time.time()))\n",
        "        train_loss= 0.0\n",
        "\n",
        "        for batch_idx, sample in enumerate(loader):\n",
        "            data = sample['data']\n",
        "            data= torch.Tensor(data)\n",
        "\n",
        "            target= sample['label']\n",
        "            target= torch.Tensor(target)\n",
        "\n",
        "            data= data.permute(1,0,2)\n",
        "            target= target.permute(1,0,2)\n",
        "\n",
        "            #print (data.shape)\n",
        "            #print (target.shape)\n",
        "            data_length= list(data.shape)[0]\n",
        "\n",
        "            data = data.to(device, dtype=torch.float)\n",
        "            target = target.to(device, dtype=torch.float)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output1, output2 = net(data)\n",
        "            #print (output1)\n",
        "            #print (output2)\n",
        "\n",
        "            #print (output1.shape)\n",
        "            #print (output2.shape)\n",
        "\n",
        "            total_loss= criterion_onset(output1, torch.narrow(target, dim= 2, start= 0, length= 2))\n",
        "            total_loss= total_loss+ criterion_pitch(output2, torch.narrow(target, dim= 2, start= 2, length= 1))\n",
        "            train_loss= train_loss+ total_loss.item()\n",
        "            total_length= total_length+ 1\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print (\"epoch %d, sample %d, loss %.6f\" %(epoch, batch_idx, total_loss))\n",
        "                #print (\"current time: %f\" %(time.time()))\n",
        "                sys.stdin.flush()\n",
        "        print('epoch %d, avg loss: %.6f' %(epoch, train_loss/ total_length))\n",
        "        model_path= f'ori_{epoch}.pt'\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "    return net\n",
        "\n",
        "def preprocess(data_seq, label):\n",
        "    new_label= []\n",
        "    for i in range(len(label)):\n",
        "        label_of_one_song= []\n",
        "        cur_note= 0\n",
        "        cur_note_onset= label[i][cur_note][0]\n",
        "        cur_note_offset= label[i][cur_note][1]\n",
        "        cur_note_pitch= label[i][cur_note][2]\n",
        "\n",
        "        for j in range(len(data_seq[i])):\n",
        "            cur_time= j* 0.032+ 0.016\n",
        "        \n",
        "            if abs(cur_time - cur_note_onset) < 0.017:\n",
        "                label_of_one_song.append(np.array([1, 0, cur_note_pitch]))\n",
        "\n",
        "            elif cur_time < cur_note_onset or cur_note >= len(label[i]):\n",
        "                label_of_one_song.append(np.array([0, 0, 0.0]))\n",
        "\n",
        "            elif abs(cur_time - cur_note_offset) < 0.017:\n",
        "                label_of_one_song.append(np.array([0, 1, cur_note_pitch]))\n",
        "                cur_note= cur_note+ 1\n",
        "                if cur_note < len(label[i]):\n",
        "                    cur_note_onset= label[i][cur_note][0]\n",
        "                    cur_note_offset= label[i][cur_note][1]\n",
        "                    cur_note_pitch= label[i][cur_note][2]\n",
        "            else:\n",
        "                label_of_one_song.append(np.array([0, 0, cur_note_pitch]))\n",
        "\n",
        "        new_label.append(label_of_one_song)\n",
        "\n",
        "    return new_label\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    '''\n",
        "    THE_FOLDER = \"./MIR-ST500\"\n",
        "    data_seq= []\n",
        "    label= []\n",
        "\n",
        "    for the_dir in os.listdir(THE_FOLDER):\n",
        "        #print (the_dir)\n",
        "        if not os.path.isdir(THE_FOLDER + \"/\" + the_dir):\n",
        "            continue\n",
        "\n",
        "        json_path = THE_FOLDER + \"/\" + the_dir+ f\"/{the_dir}_feature.json\"\n",
        "        gt_path= THE_FOLDER+ \"/\" +the_dir+ \"/\"+ the_dir+ \"_groundtruth.txt\"\n",
        "        youtube_link_path= THE_FOLDER+ \"/\" + the_dir+ \"/\"+ the_dir+ \"_link.txt\"\n",
        "\n",
        "        with open(json_path, 'r') as json_file:\n",
        "            temp = json.loads(json_file.read())\n",
        "\n",
        "        gtdata = np.loadtxt(gt_path)\n",
        "\n",
        "        data= []\n",
        "        for key, value in temp.items():\n",
        "            data.append(value)\n",
        "\n",
        "        data= np.array(data).T\n",
        "\n",
        "        data_seq.append(data)\n",
        "        label.append(gtdata)\n",
        "    \n",
        "    label= preprocess(data_seq, label)\n",
        "    train_data = MyData(data_seq, label)\n",
        "    \n",
        "    with open(\"train_all_2.pkl\", 'wb') as pkl_file:\n",
        "        pickle.dump(train_data, pkl_file)\n",
        "    '''\n",
        "    with open(\"train_all_2.pkl\", 'rb') as pkl_file:\n",
        "        train_data= pickle.load(pkl_file)\n",
        "\n",
        "    input_dim= 23\n",
        "    hidden_size= 50\n",
        "    BATCH_SIZE= 1\n",
        "    loader = Data.DataLoader(dataset=train_data, batch_size= BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    model = Myrnn(input_dim, hidden_size)\n",
        "    optimizer = optim.Adam(model.parameters(), lr= 0.001)\n",
        "\n",
        "    device = 'cpu'\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "    else: \n",
        "        device = 'cpu'\n",
        "    print(\"use\",device,\"now!\")\n",
        "\n",
        "    model.to(device)\n",
        "    model= do_training(model, loader, optimizer, device)\n",
        "    #for testing\n",
        "    \n",
        "    #model.load_state_dict(torch.load(\"ST_5.pt\"))\n",
        "    #testing(model, train_data[0], device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "use cuda now!\n",
            "epoch 0 start time: 1591606475.887910\n",
            "epoch 0, sample 0, loss 33.396461\n",
            "epoch 0, sample 50, loss 39.438618\n",
            "epoch 0, sample 100, loss 31.910355\n",
            "epoch 0, sample 150, loss 21.469341\n",
            "epoch 0, sample 200, loss 20.027613\n",
            "epoch 0, sample 250, loss 19.050505\n",
            "epoch 0, sample 300, loss 27.896547\n",
            "epoch 0, sample 350, loss 27.752306\n",
            "epoch 0, sample 400, loss 19.586676\n",
            "epoch 0, sample 450, loss 10.502892\n",
            "epoch 0, avg loss: 23.511831\n",
            "epoch 1 start time: 1591609315.619879\n",
            "epoch 1, sample 0, loss 9.961915\n",
            "epoch 1, sample 50, loss 7.468236\n",
            "epoch 1, sample 100, loss 10.143873\n",
            "epoch 1, sample 150, loss 7.004801\n",
            "epoch 1, sample 200, loss 7.569450\n",
            "epoch 1, sample 250, loss 6.306088\n",
            "epoch 1, sample 300, loss 6.442085\n",
            "epoch 1, sample 350, loss 4.957866\n",
            "epoch 1, sample 400, loss 7.222989\n",
            "epoch 1, sample 450, loss 9.548290\n",
            "epoch 1, avg loss: 8.724917\n",
            "epoch 2 start time: 1591612149.654126\n",
            "epoch 2, sample 0, loss 6.955619\n",
            "epoch 2, sample 50, loss 12.960403\n",
            "epoch 2, sample 100, loss 7.016714\n",
            "epoch 2, sample 150, loss 9.886578\n",
            "epoch 2, sample 200, loss 8.688577\n",
            "epoch 2, sample 250, loss 4.056586\n",
            "epoch 2, sample 300, loss 6.941922\n",
            "epoch 2, sample 350, loss 10.148137\n",
            "epoch 2, sample 400, loss 5.663525\n",
            "epoch 2, sample 450, loss 8.802322\n",
            "epoch 2, avg loss: 7.899040\n",
            "epoch 3 start time: 1591615001.526214\n",
            "epoch 3, sample 0, loss 11.088967\n",
            "epoch 3, sample 50, loss 6.823836\n",
            "epoch 3, sample 100, loss 9.222741\n",
            "epoch 3, sample 150, loss 14.699584\n",
            "epoch 3, sample 200, loss 6.303449\n",
            "epoch 3, sample 250, loss 6.587665\n",
            "epoch 3, sample 300, loss 9.935145\n",
            "epoch 3, sample 350, loss 7.736321\n",
            "epoch 3, sample 400, loss 9.323412\n",
            "epoch 3, sample 450, loss 4.815748\n",
            "epoch 3, avg loss: 7.885971\n",
            "epoch 4 start time: 1591617847.975257\n",
            "epoch 4, sample 0, loss 10.712118\n",
            "epoch 4, sample 50, loss 7.998049\n",
            "epoch 4, sample 100, loss 14.726957\n",
            "epoch 4, sample 150, loss 12.231478\n",
            "epoch 4, sample 200, loss 8.811646\n",
            "epoch 4, sample 250, loss 7.160942\n",
            "epoch 4, sample 300, loss 5.579989\n",
            "epoch 4, sample 350, loss 9.510586\n",
            "epoch 4, sample 400, loss 6.933822\n",
            "epoch 4, sample 450, loss 4.926214\n",
            "epoch 4, avg loss: 7.921352\n",
            "epoch 5 start time: 1591620690.672255\n",
            "epoch 5, sample 0, loss 8.239767\n",
            "epoch 5, sample 50, loss 8.044526\n",
            "epoch 5, sample 100, loss 6.618763\n",
            "epoch 5, sample 150, loss 5.720617\n",
            "epoch 5, sample 200, loss 11.417531\n",
            "epoch 5, sample 250, loss 9.754827\n",
            "epoch 5, sample 300, loss 9.319247\n",
            "epoch 5, sample 350, loss 7.378680\n",
            "epoch 5, sample 400, loss 9.451895\n",
            "epoch 5, sample 450, loss 10.542699\n",
            "epoch 5, avg loss: 7.871687\n",
            "epoch 6 start time: 1591623529.025904\n",
            "epoch 6, sample 0, loss 10.633995\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}